# many to one stacked

- convolution 에서 layer를 여러개 쌓듯이 recurrent neural network를 여러개 쌓을 수 있습니다.
- 이를 multi layered RNN 또는 Stacked RNN 이라고 합니다.

- 성능 : shallow RNN < stacked RNN 

- Semantic Information:  의미적 정보
- Syntactic Information: 문법적 정보

- Stacked RNN
  - input 에 가까운 hidden states : 문법적 정보를 파악하는 데 더 좋은 성능을 갖추었다.
  - output 에 가까운 hidden states : 문장의 의미를 찾는데 더 좋은 성능을 갖추었다.

## sentence classification

- example data
 ```
sentences = ['What I cannot create, I do not understand.',
             'Intellecuals solve problems, geniuses prevent them',

             'A person who never made a mistake never tied anything new.',

             'The same equations have the same solutions.']
y_data = [1,0,0,1] # 1: richard feynman, 0: albert einstein
 ```

 - creating a token dictionary
 ```
char_set = ['<pad>'] + sorted(list(set(''.join(sentences))))
idx2char = {idx : char for idx, char in enumerate(char_set)}
char2idx = {char : idx for idx, char in enumerate(char_set)}

print(char_set)
print(idx2char)
print(char2idx)
 ```
 - 왜 sorted function을 이용할까? 라는 생각이 들었습니다.
   - 어차피 sorted해서 숫자만 붙여준 후에
   - 밑에서 붙여준 숫자를 이용해서 문장순서대로 사용하기때문에
   - 문장의 순서를 뒤바꾼게 아니라 숫자붙일때만 정렬한것이라 상관없다!라는 결론이 나왔습니다.

- converting sequence of tokens to sequence of indices
 ```
x_data = list(map(lambda sentence : [char2idx.get(char) for char in sentence], sentences))
x_data_len = list(map(lambda sentence : len(sentence), sentences))

print(x_data)
print(x_data_len)
print(y_data)

 ```

### 길이가 긴 시퀀스

- 길이가 긴 시퀀스를 다룰때는 단순한 RNN보다 LSTM(long short-term memory network) 혹은 GRU(Grated Recurrent Unit) 을 사용할수있습니다.

### pad_sequences

- from tensorflow.keras.preprocessing.sequence import pad_sequences
- max_sequence 가 가리키는 값 만큼의 길이로 변환한 데이터를 padding합니다.

## model

- embedding layer : 각각의 토큰을 one hot vector로 표현합니다.
  - mask_zero = True 옵션으로 시퀀스중 0으로 패딩된 부분을 연산에 포함하지 않습니다.
  - trainable = False 옵션으로 one hot vector를 트레이닝 하지 않을수 있습니다.
- SimpleRNN
  - return_sequences=True : 두번째 RNN이 필요한 형태입니다.
  - (data_dimension, max_sequences, input_dim) 형태로 데이터를 리턴합니다.
- TimeDistributed
  - stacked RNN 은 shallow RNN에 비해 모델의 capacity가 높은 구조 : `오버피팅될 가능성 큽니다.`
  - dropout
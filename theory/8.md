# Seq to Seq with attention

- Seq2Seq 의 문제
  - 한개의 벡터값 으로 decoder 학습을 진행하면
    - 모든 정보를 담기 어렵다
    - 입력언어와 타겟언어가 길면 길어질수록 모델의 성능이 떨어짐

## Seq to Seq Attention

- Attention
  - 중요한 문장이나 단어에 집중합니다.

### 이전 Seq2Seq

- 마지막 인코더의 hidden state 를 활용하여 컨텍스트 벡터를 생성하고 그것을 기반으로 학습하였습니다.

### with Attention

- 매 인코더의 스텝을 전부 활용하여 컨텍스트 벡터를 생성
- 가장 중요하게 영향을 미친 인코더에 가중치(attention weights)
